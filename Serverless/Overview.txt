Serverless
    - we dont need to manage servers.
    - we have following resources that are believed to be Serverless
        - AWS lambda
        - amazon s3
        - dynamo db
        - aws cognito
        - aws api gateway
        - sns and sqs
        - aws kinesis and data firehose
        - aurora Serverless
        - step functions
        - fargate

AWS lambda
    - virtual functions - no servers to manage
    - limited by time - short executions
    - run on-demand
    - scaling is automated.
    - pay per request and compute time , also have free tiers.
    - has easy monitoring through cloudwatch
    - can use 10 gb of ram per function.
    - increasing RAM will also improve CPU and network
    - it can be integrated with many other aws resources.

AWS Lambda has several important limits to know per region, covering concurrency, execution, payload, and scaling:

    - Concurrency Limits:

        Default maximum concurrent executions per region is 1,000 across all functions.
        You can request an increase if needed.
        Each function can scale up to 1,000 new execution environments every 10 seconds (or 10,000 requests per second per 10 seconds).
        Requests beyond concurrency limits are throttled with error 429.​

    - Execution Limits:

        Maximum execution timeout per function is 15 minutes (900 seconds).
        Functions must complete within this time or they time out.​

    - Memory Allocation:

        Memory can be allocated from 128 MB up to 10 GB in 1 MB increments.
        CPU and network scale proportionally with memory allocated.​

    - Payload Size Limits:

        Synchronous invocation payload size limit is 6 MB.
        Asynchronous payload size is 256 KB.​

    - Ephemeral Storage:

        Each function gets 512 MB to 10 GB of temporary storage in /tmp during execution.​

    - API Request Limits:

        Limits on control plane API requests such as GetFunction (100 requests/sec) and others exist and cannot be increased.​

    - Other Limits:

        Environment variables max size is 4 KB.
        There are limits on elastic network interfaces when integrated with VPCs

Lambda Concurrency
    - we can have reserved concurrency to make sure only that much concurrent lambda function executions take place.
    - concurrency gets applied to all the functions in your account so the limits need to be set or else requests wil be throttled
    - we can use provisioned concurrency to avoid cold starts , which means slower experience during initialization.

Lambda SnapStart
    - Improves your lambda functions performance up to 10x at no extra cost for python, java and .net
    
We have Cloudfront functions and lambda@edge to serve requests faster using processing at edge.

Lambda in VPC
    - By default lambda is launched outside of your own VPC.
    - therefore it cannot access resources in your VPC.
    - for the connection to happen we need to add the VPC ID , subnets and security groups to lambda functions.
    - lambda will create an ENI(elastic network interface) in your subnets.

Lambda with RDS Proxy
    - if lambda functions directly access your database , they may open too many connections under high load.
    - so rds proxy is used , which pulls connections and connects less to rds database instance.
    - for this the lambda function need to be deployed in your VPC , because rds proxy is never publicly accessible.

Invoking lambda from RDS and Aurora
    - they can invoke lambda function to perform certain task but this needs required permissions and also outbound traffic allowed to lambda function.